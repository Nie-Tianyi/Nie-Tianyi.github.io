<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>神经网络梯度推导 | nty的技术博客</title><meta name="author" content="nty"><meta name="copyright" content="nty"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="正向传播 假设我们的神经网络是由三层神经元构成：分别是一层拥有4个神经元的隐藏层，一层拥有三个神经元的隐藏层，以及最后一层只拥有一个神经元的输出层，神经网络结构如下图所示：  假设我们输入的数据的维度是2，那么第一层神经元里面的每一个神经元都有$ w_1 和和和 w_2 $两个权重以及一个偏置，整个第一层就一共有 $ 2 \times 4 &#x3D; 8 个个个 w 以及四个偏置以及四个偏置以及四个偏置">
<meta property="og:type" content="article">
<meta property="og:title" content="神经网络梯度推导">
<meta property="og:url" content="https://nie-tianyi.github.io/2025/08/17/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C's%20Gradient/index.html">
<meta property="og:site_name" content="nty的技术博客">
<meta property="og:description" content="正向传播 假设我们的神经网络是由三层神经元构成：分别是一层拥有4个神经元的隐藏层，一层拥有三个神经元的隐藏层，以及最后一层只拥有一个神经元的输出层，神经网络结构如下图所示：  假设我们输入的数据的维度是2，那么第一层神经元里面的每一个神经元都有$ w_1 和和和 w_2 $两个权重以及一个偏置，整个第一层就一共有 $ 2 \times 4 &#x3D; 8 个个个 w 以及四个偏置以及四个偏置以及四个偏置">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://nie-tianyi.github.io/images/avatar.png">
<meta property="article:published_time" content="2025-08-17T08:36:27.000Z">
<meta property="article:modified_time" content="2025-09-14T07:27:06.755Z">
<meta property="article:author" content="nty">
<meta property="article:tag" content="machine learning">
<meta property="article:tag" content="neural network">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://nie-tianyi.github.io/images/avatar.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "神经网络梯度推导",
  "url": "https://nie-tianyi.github.io/2025/08/17/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C's%20Gradient/",
  "image": "https://nie-tianyi.github.io/images/avatar.png",
  "datePublished": "2025-08-17T08:36:27.000Z",
  "dateModified": "2025-09-14T07:27:06.755Z",
  "author": [
    {
      "@type": "Person",
      "name": "nty",
      "url": "https://nie-tianyi.github.io/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://nie-tianyi.github.io/2025/08/17/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C's%20Gradient/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '神经网络梯度推导',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head><body><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">nty的技术博客</span></a><a class="nav-page-title" href="/"><span class="site-name">神经网络梯度推导</span></a></span><div id="menus"></div></nav><div id="post-info"><h1 class="post-title">神经网络梯度推导</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-08-17T08:36:27.000Z" title="发表于 2025-08-17 16:36:27">2025-08-17</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-09-14T07:27:06.755Z" title="更新于 2025-09-14 15:27:06">2025-09-14</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h2 id="正向传播">正向传播</h2>
<p>假设我们的神经网络是由三层神经元构成：分别是一层拥有4个神经元的隐藏层，一层拥有三个神经元的隐藏层，以及最后一层只拥有一个神经元的输出层，神经网络结构如下图所示：</p>
<p><img src="https://cdn.nlark.com/yuque/0/2025/png/45785720/1754758666679-554e9a17-89aa-44b0-abb8-52af05e59b09.png" alt></p>
<p>假设我们输入的数据的维度是2，那么第一层神经元里面的每一个神经元都有$ w_1 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>和</mtext></mrow><annotation encoding="application/x-tex">和</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">和</span></span></span></span> w_2 $两个权重以及一个偏置，整个第一层就一共有 $ 2 \times 4 = 8 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>个</mtext></mrow><annotation encoding="application/x-tex">个</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">个</span></span></span></span> w <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>以及四个偏置</mtext></mrow><annotation encoding="application/x-tex">以及四个偏置</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">以及四个偏置</span></span></span></span> b <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>。我们用</mtext></mrow><annotation encoding="application/x-tex">。我们用</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">。我们用</span></span></span></span> w^{(l)}_{ij} $表示每一层的权重，其中</p>
<ul>
<li>$ (l) $表示这个权重属于第几层。</li>
<li>$ i $表示这是这一层的第几个神经元。</li>
<li>最后$ j $表示这是这个神经元的第几个权重，也对应着前一层第几个神经元的输出。</li>
</ul>
<p>例如，第一层的第一个神经元的权重为$ w_{11}^{(1)} <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>以及</mtext></mrow><annotation encoding="application/x-tex">以及</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">以及</span></span></span></span> w_{12}^{(1)} $，这里的上标（1）代表了这两个权重是第一层神经元的权重；而下标的两个数字，前面一个数字“1”代表了这个第一层的第一个神经元，第二个数字“1”和“2”分别代表了这是这一个神经元的第一个权重和第二个权重。</p>
<blockquote>
<p>在机器学习中我们还通常习惯性的把输入的数据叫做第零层，第零层通常不算做神经网络的层数。</p>
</blockquote>
<p>第二层神经元里面每一个神经元都有四个$ w <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>——每个</mtext></mrow><annotation encoding="application/x-tex">——每个</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">——</span><span class="mord cjk_fallback">每个</span></span></span></span> w <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>分别对应着上一层神经元的四个输出，以及一个偏置</mtext></mrow><annotation encoding="application/x-tex">分别对应着上一层神经元的四个输出，以及一个偏置</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">分别对应着上一层神经元的四个输出，以及一个偏置</span></span></span></span> b^{(2)} $。而最后一层输出层的参数数量则根据需要预测的目标来决定：如果我们需要这个神经网络预测房价，一个线性预测任务，则输出层有一个神经元，神经元内有有三个权重，分别对应着上一层的三个输出；如果是决定某样商品是否畅销，一个二分类任务，则输出层应该有一个神经元，神经元内有三个权重以及Sigmoid激活函数；如果我们要识别MNIST中的手写数字，则输出层应该有10个神经元，并且使用Softmax函数作为最后一层的激活函数。</p>
<table>
<thead>
<tr>
<th>任务</th>
<th>输出层神经元数量</th>
<th>输出层激活函数</th>
<th>损失函数</th>
</tr>
</thead>
<tbody>
<tr>
<td>房价</td>
<td>1</td>
<td>无/线性，y=x</td>
<td>均方误差（MSE）</td>
</tr>
<tr>
<td>二分类问题</td>
<td>1</td>
<td>Sigmoid</td>
<td>交叉熵损失函数（CEL）</td>
</tr>
<tr>
<td>多分类问题（例如MNIST）</td>
<td>K（MNIST中K=10）</td>
<td>Softmax</td>
<td><font style="color:rgb(64, 64, 64);">分类交叉熵</font></td>
</tr>
</tbody>
</table>
<p>那么神经网络是怎么做出预测的呢？神经网络会逐层的计算每一层的结果，最后到输出层，我们把这个过程叫做正向传播（或者前向传播，forward propagation）。在上例中，我们的神经网络的输入为一个二维的数据，假设为$ x_1 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>和</mtext></mrow><annotation encoding="application/x-tex">和</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">和</span></span></span></span> x_2 $，那么我们首先计算第一层神经元的输出：</p>
<p>第一层有四个神经元，我们分别计算每个神经元与我们的输入相乘：</p>
<p>$ z^{(1)}<em>1 = w^{(1)}</em>{11}x_1 + w^{(1)}_{12}x_2 + b^{(1)}_1 $</p>
<p>$ z^{(1)}<em>2 = w^{(1)}</em>{21}x_1 + w^{(1)}_{22}x_2 + b^{(1)}_2 $</p>
<p>$ z^{(1)}<em>3 = w^{(1)}</em>{31}x_1 + w^{(1)}_{32}x_2 + b^{(1)}_3 $</p>
<p>$ z^{(1)}<em>4 = w^{(1)}</em>{41}x_1 + w^{(1)}_{42}x_2 + b^{(1)}_4 $</p>
<p>然后对每一个输出应用激活函数 $ h(x) $：</p>
<p>$ a^{(1)}_1 = h(z^{(1)}_1) \<br>
… \<br>
a^{(1)}_4 = h(z^{(1)}_4)<br>
$</p>
<p>现在我们有了$ a^{(1)}_1 … a^{(1)}_4 $四个输出，在全连接神经网络中，这一层的所有神经元的输出，都将作为下一层神经元的输入。与之相对的是稀疏连接神经网络，在稀疏连接神经网络中，下一层的每一个神经元只连接上一层中离其较近的几个神经元。稀疏神经网络较全连接神经网络相比，需要计算的数据更少，在特定数据集上的表现要优于全连接神经网络。在这里，我们以最常见的全连接神经网络为例，计算下一层的输出——在第二层中，我们一共有三个神经元，每一个神经元都接收来自上一层四个神经元的输出，我们计算每一个神经元的权重与上一层输出的乘积加偏置：</p>
<p>$ z^{(2)}<em>1 = w^{(2)}</em>{11}a^{(1)}<em>1 + w^{(2)}</em>{12}a^{(1)}<em>2 + w^{(2)}</em>{13}a^{(1)}<em>3 + w^{(2)}</em>{14}a^{(1)}_4 + b^{(2)}_1 $</p>
<p>$ z^{(2)}<em>2 = w^{(2)}</em>{21}a^{(1)}<em>1 + w^{(2)}</em>{22}a^{(1)}<em>2 + w^{(2)}</em>{23}a^{(1)}<em>3 + w^{(2)}</em>{24}a^{(1)}_4 + b^{(2)}_2 $</p>
<p>$ z^{(2)}<em>3 = w^{(2)}</em>{31}a^{(1)}<em>1 + w^{(2)}</em>{32}a^{(1)}<em>2 + w^{(2)}</em>{33}a^{(1)}<em>3 + w^{(2)}</em>{34}a^{(1)}_4 + b^{(2)}_3 $</p>
<p>然后对每一个输出应用激活函数$ h(x) $</p>
<p>$ a^{(2)}_1 = h(z^{(2)}_1) \<br>
… \<br>
a^{(2)}_3 = h(z^{(2)}_3)  $</p>
<p>第二层神经元输出了 $ a^{(2)}_1 , a^{(2)}_2, a^{(2)}_3 $三个输出，这些输出会用送到最后一层输出层神经元，用于做最后的决策。在这里，我们假设我们面对的问题是一个二分类问题，那么最后一个层输出层应该只有一个神经元，并且使用激活函数Softmax——最后我们计算：</p>
<p>$ z^{(3)}<em>1 = w^{(3)}</em>{11} a^{(2)}<em>1 + w^{(3)}</em>{12} a^{(2)}<em>2 + w^{(3)}</em>{13} a^{(2)}_3 + b^{(3)}_1 $</p>
<p>最后使用Softmax函数，输出一个预测标签为1的概率：</p>
<p>$ \hat{y} = \frac{1}{1 + e^{-z^{(3)}_1}} $</p>
<p>如果我们面对的是一个多分类问题，则最后一层应该有 K 个神经元（K为实际标签类别的数量），然后再在K个神经元的输出上使用 Softmax 激活函数。</p>
<h2 id="反向传播">反向传播</h2>
<p>接着我们更新权重参数，我们分别计算损失函数在每一个权重和偏置方向上的偏导。如果我们要预测的目标是一个线性问题，那么我们的损失函数为均方差MSE函数；如果我们需要预测的目标是一个二分类问题，那么我们的损失函数则应该是交叉熵损失函数CEL；如果是多分类问题，则应该是是稀疏交叉熵函数SCEL。</p>
<p>在这里，我们先以二分类问题举例，假设我们的损失函数为交叉熵损失函数：</p>
<p>$ L(\hat{y},y) = -y\times log(\hat{y}) - (1-y)\times log(1-\hat{y}) $</p>
<p>我们先计算输出层$ w_{11}^{(3)} <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>的梯度（记住</mtext></mrow><annotation encoding="application/x-tex">的梯度（记住</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">的梯度（记住</span></span></span></span> w_{11}^{(3)} $为输出层第三层的第一个神经元的第一个参数）：</p>
<p>$ \begin{aligned}<br>
\frac{\partial L(\hat{y},y)}{\partial w^{(3)}_{11}} &amp;= \frac{\partial L(\hat{y},y)}{\partial\hat{y}} \cdot \frac{\partial \hat{y}}{\partial z^{(3)}_1} \cdot \frac{\partial z^{(3)}<em>1}{\partial w^{(3)}</em>{11}} \<br>
&amp;= (-\frac{y}{\hat{y}} + \frac{1-y}{1-\hat{y}})\cdot \hat{y}(1-\hat{y})\cdot a^{(2)}_1 \</p>
<p>&amp;= (\hat{y} - y) \cdot a^{(2)}_1<br>
\end{aligned} $</p>
<p>同理我们可以得到$ w_{12}^{(3)} <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>、</mtext></mrow><annotation encoding="application/x-tex">、</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">、</span></span></span></span> w_{13}^{(3)} <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>以及</mtext></mrow><annotation encoding="application/x-tex">以及</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">以及</span></span></span></span> b^{(3)}_1 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>的梯度，分别为</mtext></mrow><annotation encoding="application/x-tex">的梯度，分别为</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">的梯度，分别为</span></span></span></span> (\hat{y} - y) \cdot a^{(2)}_2 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>、</mtext></mrow><annotation encoding="application/x-tex">、</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">、</span></span></span></span> (\hat{y} - y) \cdot a^{(2)}<em>3 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>和</mtext></mrow><annotation encoding="application/x-tex">和</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">和</span></span></span></span> (\hat{y} - y) $ 。接下来我们求隐藏层的参数的梯度，我们先求$ w^{(2)}</em>{11} $的梯度：</p>
<p>$ \begin{aligned}<br>
\frac{\partial L(\hat{y},y)}{\partial w^{(2)}_{11}} &amp;= \frac{\partial L(\hat{y},y)}{\partial\hat{y}} \cdot \frac{\partial \hat{y}}{\partial z^{(3)}_1} \cdot \frac{\partial z^{(3)}_1}{\partial a^{(2)}_1} \cdot \frac{\partial a^{(2)}<em>1}{\partial z^{(2)}<em>1} \cdot \frac{\partial z^{(2)}<em>1}{\partial w^{(2)}</em>{11}} \<br>
&amp;= (\hat{y}-y) \cdot w^{(3)}</em>{11} \cdot h’(z^{(2)}</em>{1}) \cdot a^{(1)}_1</p>
<p>\end{aligned} $</p>
<p>同理可得$ w^{(2)}<em>{12} <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>、</mtext></mrow><annotation encoding="application/x-tex">、</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">、</span></span></span></span> w^{(2)}</em>{13} <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>、</mtext></mrow><annotation encoding="application/x-tex">、</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">、</span></span></span></span> w^{(2)}_{14} <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>以及</mtext></mrow><annotation encoding="application/x-tex">以及</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">以及</span></span></span></span> b^{(2)}_1 $的梯度分别为：</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>梯度</th>
</tr>
</thead>
<tbody>
<tr>
<td>$ w^{(2)}_{12} $</td>
<td>$ (\hat{y}-y) \cdot w^{(3)}<em>{11} \cdot h’(z^{(2)}</em>{1}) \cdot a^{(1)}_2 $</td>
</tr>
<tr>
<td>$ w^{(2)}_{13} $</td>
<td>$ (\hat{y}-y) \cdot w^{(3)}<em>{11} \cdot h’(z^{(2)}</em>{1}) \cdot a^{(1)}_3 $</td>
</tr>
<tr>
<td>$ w^{(2)}_{14} $</td>
<td>$ (\hat{y}-y) \cdot w^{(3)}<em>{11} \cdot h’(z^{(2)}</em>{1}) \cdot a^{(1)}_4 $</td>
</tr>
<tr>
<td>$ b^{(2)}_1 $</td>
<td>$ (\hat{y}-y) \cdot w^{(3)}<em>{11} \cdot h’(z^{(2)}</em>{1}) $</td>
</tr>
</tbody>
</table>
<p>我们再计算$ w_{21}^{(2)} $的梯度：</p>
<p>$ \begin{aligned}<br>
\frac{\partial L(\hat{y},y)}{\partial w^{(2)}_{21}} &amp;= \frac{\partial L(\hat{y},y)}{\partial\hat{y}} \cdot \frac{\partial \hat{y}}{\partial z^{(3)}_1} \cdot \frac{\partial z^{(3)}_1}{\partial a^{(2)}_2} \cdot \frac{\partial a^{(2)}<em>2}{\partial z^{(2)}<em>2} \cdot \frac{\partial z^{(2)}<em>2}{\partial w^{(2)}</em>{21}} \<br>
&amp;= (\hat{y}-y) \cdot w^{(3)}</em>{12} \cdot h’(z^{(2)}</em>{2}) \cdot a^{(1)}_1</p>
<p>\end{aligned} $</p>
<p>同理我们也可以得到$ w^{(2)}_{31} $的梯度：</p>
<p>$ \frac{\partial L(\hat{y},y)}{\partial w^{(2)}<em>{31}}<br>
= (\hat{y}-y) \cdot w^{(3)}</em>{13} \cdot h’(z^{(2)}_{3}) \cdot a^{(1)}_1 $</p>
<p>我们发现在计算第二层神经网络的参数的时候，我们都要计算$ \frac{\partial L(\hat{y},y)}{\partial\hat{y}} \cdot \frac{\partial \hat{y}}{\partial z^{(3)}_1} = \hat{y} - y $，于是我们令:</p>
<p>$ \delta^{(3)}_1 = \frac{\partial L(\hat{y},y)}{\partial\hat{y}} \cdot \frac{\partial \hat{y}}{\partial z^{(3)}_1} = \hat{y} - y $</p>
<p>$ \delta_i^{(2)} = \delta^{(3)}<em>1 w^{(3)}</em>{1i} h’(z^{(2)}_{i}) $</p>
<table>
<thead>
<tr>
<th>梯度</th>
<th>$ w_{i1}^{(2)} $</th>
<th>$ w_{i2}^{(2)} $</th>
<th>$ w_{i3}^{(2)} $</th>
<th>$ w_{i4}^{(2)} $</th>
<th>$ b_{i}^{(2)} $</th>
</tr>
</thead>
<tbody>
<tr>
<td>i=1</td>
<td>$ \delta_1^{(2)} a^{(1)}_1 $</td>
<td>$ \delta_1^{(2)} a^{(1)}_2 $</td>
<td>$ \delta_1^{(2)}  a^{(1)}_3 $</td>
<td>$ \delta_1^{(2)}  a^{(1)}_4 $</td>
<td>$ \delta_1^{(2)}  $</td>
</tr>
<tr>
<td>i=2</td>
<td>$ \delta_2^{(2)}  a^{(1)}_1 $</td>
<td>$ \delta_2^{(2)}   a^{(1)}_2 $</td>
<td>$ \delta_2^{(2)}  a^{(1)}_3 $</td>
<td>$ \delta_2^{(2)}   a^{(1)}_4 $</td>
<td>$ \delta_2^{(2)}  $</td>
</tr>
<tr>
<td>i=3</td>
<td>$ \delta_3^{(2)}   a^{(1)}_1 $</td>
<td>$ \delta_3^{(2)}   a^{(1)}_2 $</td>
<td>$ \delta_3^{(2)}   a^{(1)}_3 $</td>
<td>$ \delta_3^{(2)}  a^{(1)}_4 $</td>
<td>$ \delta_3^{(2)}   $</td>
</tr>
</tbody>
</table>
<p>通用的公式可以写为：</p>
<p>$ \frac{\partial L(\hat{y},y)}{\partial w^{(2)}_{ij}} =\delta_i^{(2)} a^{(1)}_j $</p>
<p>最后我们求第一层神经网络参数的梯度，我们先求$ w_{11}^{(1)} <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>的梯度。先回忆一下，</mtext></mrow><annotation encoding="application/x-tex">的梯度。先回忆一下，</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">的梯度。先回忆一下，</span></span></span></span> w_{11}^{(1)} <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>通过和输入</mtext></mrow><annotation encoding="application/x-tex">通过和输入</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">通过和输入</span></span></span></span> x_1 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>相乘得到</mtext></mrow><annotation encoding="application/x-tex">相乘得到</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">相乘得到</span></span></span></span> z_{1}^{(1)} <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">,</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3em;vertical-align:-0.1944em;"></span><span class="mpunct">,</span></span></span></span> z_{1}^{(1)} <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>通过激活函数得到</mtext></mrow><annotation encoding="application/x-tex">通过激活函数得到</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">通过激活函数得到</span></span></span></span> a_1^{(1)} $。</p>
<p>$ \begin{aligned}<br>
\frac{\partial L(\hat{y}, y)}{\partial w_{11}^{(1)}} &amp;= \frac{\partial L(\hat{y}, y)}{\partial a_{1}^{(1)}} \cdot \frac{\partial a_{1}^{(1)}}{\partial z_1^{(1)}} \cdot \frac{\partial z_1^{(1)}}{\partial w_{11}^{(1)}} \<br>
&amp;= \frac{\partial L(\hat{y}, y)}{\partial a_{1}^{(1)}} \cdot h’(z_1^{(1)}) \cdot x_1<br>
\end{aligned} $</p>
<p>然后第二层神经网络内三个神经元都使用了$ a_1^{(1)} <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>进行了计算，分别得到了</mtext></mrow><annotation encoding="application/x-tex">进行了计算，分别得到了</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">进行了计算，分别得到了</span></span></span></span> z_1^{(2)} <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>、</mtext></mrow><annotation encoding="application/x-tex">、</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">、</span></span></span></span> z_2^{(2)} <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>、</mtext></mrow><annotation encoding="application/x-tex">、</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">、</span></span></span></span> z_3^{(2)} <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>，并且最后都对最终结果产生了影响。接着我们单独计算</mtext></mrow><annotation encoding="application/x-tex">，并且最后都对最终结果产生了影响。接着我们单独计算</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">，并且最后都对最终结果产生了影响。接着我们单独计算</span></span></span></span> \frac{\partial L(\hat{y}, y)}{\partial a_{1}^{(1)}} $:</p>
<p>$ \begin{aligned}<br>
\frac{\partial L(\hat{y}, y)}{\partial a_{1}^{(1)}} &amp;= \frac{\partial L(\hat{y}, y)}{\partial z_1^{(2)}} \cdot \frac{\partial z_1^{(2)}}{\partial a_1^{(1)}} +  \frac{\partial L(\hat{y}, y)}{\partial z_2^{(2)}} \cdot \frac{\partial z_2^{(2)}}{\partial a_1^{(1)}} +  \frac{\partial L(\hat{y}, y)}{\partial z_3^{(2)}} \cdot \frac{\partial z_3^{(2)}}{\partial a_1^{(1)}} \</p>
<p>&amp;= \sum_{k=1}^3 \frac{\partial L(\hat{y},y)}{\partial\hat{y}} \cdot \frac{\partial \hat{y}}{\partial z^{(3)}_1} \cdot \frac{\partial z^{(3)}_1}{\partial a_k^{(2)}} \cdot \frac{\partial a^{(2)}_k}{\partial z_k^{(2)}} \cdot \frac{\partial z_k^{(2)}}{\partial a_1^{(1)}} \</p>
<p>&amp;=\sum_{k=1}^3 (\hat{y}-y) \cdot w_{1k}^{(3)} \cdot h’(z_k^{(2)}) \cdot w_{k1}^{(2)}  \<br>
&amp;= \sum_{k=1}^3 (\hat{y} - y)\cdot w_{1k}^{(3)} \cdot h’(z_k^{(2)}) \cdot w_{k1}^{(2)}<br>
\end{aligned} $</p>
<p>将这个结果带到上面的式子，就可以得到：</p>
<p>$ \begin{aligned}<br>
\frac{\partial L(\hat{y}, y)}{\partial w_{11}^{(1)}} &amp;= \frac{\partial L(\hat{y}, y)}{\partial a_{1}^{(1)}} \cdot \frac{\partial a_{1}^{(1)}}{\partial z_1^{(1)}} \cdot \frac{\partial z_1^{(1)}}{\partial w_{11}^{(1)}} \<br>
&amp;= \frac{\partial L(\hat{y}, y)}{\partial a_{1}^{(1)}} \cdot h’(z_1^{(1)}) \cdot x_1 \<br>
&amp;= \sum_{k=1}^3(\hat{y} - y)\cdot w_{1k}^{(3)} \cdot h’(z_k^{(2)}) \cdot w_{k1}^{(2)} \cdot h’(z_1^{(1)}) \cdot x_1<br>
\end{aligned} $</p>
<p>接着我们计算$ w_{12}^{(1)} <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>以及</mtext></mrow><annotation encoding="application/x-tex">以及</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">以及</span></span></span></span> b^{(1)}_1 $的梯度：</p>
<p>$ \begin{aligned}<br>
\frac{\partial L(\hat{y}, y)}{\partial w_{12}^{(1)}} &amp;= \frac{\partial L(\hat{y}, y)}{\partial a_{1}^{(1)}} \cdot \frac{\partial a_{1}^{(1)}}{\partial z_1^{(1)}} \cdot \frac{\partial z_1^{(1)}}{\partial w_{12}^{(1)}} \<br>
&amp;= \frac{\partial L(\hat{y}, y)}{\partial a_{1}^{(1)}} \cdot h’(z_1^{(1)}) \cdot x_2 \<br>
&amp;= \sum_{k=1}^3 (\hat{y} - y) \cdot w_{1k}^{(3)} \cdot h’(z_k^{(2)}) \cdot w_{k1}^{(2)} \cdot h’(z_1^{(1)}) \cdot x_2<br>
\end{aligned} $</p>
<p>$ \begin{aligned}<br>
\frac{\partial L(\hat{y}, y)}{\partial b^{(1)}<em>1} &amp;= \frac{\partial L(\hat{y}, y)}{\partial a</em>{1}^{(1)}} \cdot \frac{\partial a_{1}^{(1)}}{\partial z_1^{(1)}} \cdot \frac{\partial z_1^{(1)}}{\partial b^{(1)}<em>1} \<br>
&amp;= \frac{\partial L(\hat{y}, y)}{\partial a</em>{1}^{(1)}} \cdot h’(z_1^{(1)}) \cdot 1\<br>
&amp;= \sum_{k=1}^3 (\hat{y} - y) \cdot w_{1k}^{(3)} \cdot h’(z_k^{(2)}) \cdot w_{k1}^{(2)} \cdot h’(z_1^{(1)})<br>
\end{aligned} $</p>
<p>我们接着计算$ w_{21}^{(1)} $的梯度：</p>
<p>$ \begin{aligned}<br>
\frac{\partial L(\hat{y}, y)}{\partial w_{21}^{(1)}} &amp;= \frac{\partial L(\hat{y}, y)}{\partial a_{2}^{(1)}} \cdot \frac{\partial a_{2}^{(1)}}{\partial z_2^{(1)}} \cdot \frac{\partial z_2^{(1)}}{\partial w_{21}^{(1)}} \</p>
<p>&amp;= \frac{\partial L(\hat{y}, y)}{\partial a_{2}^{(1)}} \cdot h’(z_{2}^{(1)}) \cdot x_1 \</p>
<p>&amp;= \sum_{k=1}^{3}\frac{\partial L(\hat{y}, y)}{\partial z_k^{(2)}} \cdot \frac{\partial z_k^{(2)}}{\partial a_2^{(1)}} \cdot h’(z_{2}^{(1)}) \cdot x_1 \</p>
<p>&amp;= \sum_{k=1}^{3} \frac{\partial L(\hat{y}, y)}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z_1^{(3)}} \cdot \frac{\partial z_1^{(3)}}{\partial a_k^{(2)}} \cdot \frac{\partial  a_k^{(2)}}{\partial z_k^{(2)}} \cdot \frac{\partial z_k^{(2)}}{\partial a_2^{(1)}} \cdot h’(z_{2}^{(1)}) \cdot x_1  \</p>
<p>&amp;= \sum_{k=1}^{3} (\hat{y} - y) \cdot w_{1k}^{(3)} \cdot h’(z_k^{(2)}) \cdot w^{(2)}_{k2} \cdot h’(z_2^{(1)}) \cdot x_1<br>
\end{aligned} $</p>
<p>我们令$ \delta_i^{(1)} = \sum_{k=1}^{3} (\hat{y} - y) \cdot w_{1k}^{(3)} \cdot h’(z_k^{(2)}) \cdot w^{(2)}<em>{ki}= \sum</em>{k=1}^3 \delta_i^{(2)} w_{ki}^{(2)} $，能得到以下梯度：</p>
<table>
<thead>
<tr>
<th>梯度</th>
<th>$ w_{i1}^{(1)} $</th>
<th>$ w_{i2}^{(1)} $</th>
<th>$ b_{i}^{(1)} $</th>
</tr>
</thead>
<tbody>
<tr>
<td>i=1</td>
<td>$ \delta_1^{(1)} x_1 $</td>
<td>$ \delta_1^{(1)} x_2 $</td>
<td>$ \delta_1^{(1)} $</td>
</tr>
<tr>
<td>i=2</td>
<td>$ \delta_2^{(1)} x_1 $</td>
<td>$ \delta_2^{(1)} x_2 $</td>
<td>$ \delta_2^{(1)} $</td>
</tr>
<tr>
<td>i=3</td>
<td>$ \delta_3^{(1)} x_1 $</td>
<td>$ \delta_3^{(1)} x_2 $</td>
<td>$ \delta_3^{(1)} $</td>
</tr>
<tr>
<td>i=4</td>
<td>$ \delta_4^{(1)} x_1 $</td>
<td>$ \delta_4^{(1)} x_2 $</td>
<td>$ \delta_4^{(1)} $</td>
</tr>
</tbody>
</table>
<p>我们可以得到通用的神经网络权重梯度的计算公式，对于第$ l <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>层的第</mtext></mrow><annotation encoding="application/x-tex">层的第</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">层的第</span></span></span></span> i <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>个神经元的权重</mtext></mrow><annotation encoding="application/x-tex">个神经元的权重</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">个神经元的权重</span></span></span></span> w_{ij}^{(l)} $：</p>
<p>$ \frac{\partial L(\hat{y}, y)}{\partial w^{(l)}_{ij}} = \delta_i^{(l)} \cdot a_j^{(l-1)} $</p>
<p>其中的反向传播的误差$ \delta_i^{(l)} = (\sum_k \delta_k^{(l+1)}\cdot w_{ki}^{(l+1)})\cdot h’(z^{(l)}_i) $。</p>
<p>这里使用了三个参数$ i,j,k $,分别代表了：</p>
<ul>
<li>$ i <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>：第</mtext></mrow><annotation encoding="application/x-tex">：第</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">：第</span></span></span></span> l $层神经元的第几个神经元。</li>
<li>$ j <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>：第</mtext></mrow><annotation encoding="application/x-tex">：第</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">：第</span></span></span></span> l - 1 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>层神经元的第几个神经元，也是这一层神经元的每一个神经元的第几个</mtext></mrow><annotation encoding="application/x-tex">层神经元的第几个神经元，也是这一层神经元的每一个神经元的第几个</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">层神经元的第几个神经元，也是这一层神经元的每一个神经元的第几个</span></span></span></span> w <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>参数。如果这层神经元是第一层神经元，那么</mtext></mrow><annotation encoding="application/x-tex">参数。如果这层神经元是第一层神经元，那么</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">参数。如果这层神经元是第一层神经元，那么</span></span></span></span> j $就是输入数据的第几个维度。</li>
<li>$ k <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>：第</mtext></mrow><annotation encoding="application/x-tex">：第</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">：第</span></span></span></span> l+1 $层神经元的第几个神经元。</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://Nie-Tianyi.github.io">nty</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://nie-tianyi.github.io/2025/08/17/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C's%20Gradient/">https://nie-tianyi.github.io/2025/08/17/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C's%20Gradient/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://Nie-Tianyi.github.io" target="_blank">nty的技术博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/machine-learning/">machine learning</a><a class="post-meta__tags" href="/tags/neural-network/">neural network</a></div><div class="post-share"><div class="social-share" data-image="/images/avatar.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/06/26/PostgreSQL%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/" title="PostgreSQL使用指南"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">PostgreSQL使用指南</div></div><div class="info-2"><div class="info-item-1">安装PostgreSQL 在Mac上可以直接使用Homebrew安装： 1brew install postgresql 启动PostgreSQL服务 在MacOS上可以使用以下命令启动/停止PostgreSQL服务 12brew services start postgresql # 启动brew services stop postgresql # 关闭 在windows平台上则使用net命令： 12net start postgresql-x64-14 # 启动net stop posgresql-x64-14 # 关闭 使用PostgreSQL 在启动了PostgreSQL服务后，我们就可以使用 psql 命令行工具连接 PostgreSQL了（这个工具一般在PostgreSQL的安装目录的/bin目录下）。 直接在命令行中输入psql就能进入到默认的数据库内，或者我们可以使用 psql -U [用户名] -d [数据库名]指定用户名和要连接的数据库： 1234567nietianyi@Mac ~ % brew services start...</div></div></div></a><a class="pagination-related" href="/2025/09/03/Raft%E5%8D%8F%E8%AE%AE/" title="Raft协议"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">Raft协议</div></div><div class="info-2"><div class="info-item-1">Raft协议是经典的Crash Fault Tolerance共识协议，这个协议为了易于理解性将简化了共识协议的内容，牺牲了协议的性能。Raft协议采取单Leader架构，整体算法的瓶颈在于充当Leader的节点的CPU、内存和带宽。在极限情况下Raft的吞吐量跟常见的其他CFT共识协议（例如Multi-Paxos）不会有太大差距，理论上Multi-Paxos协议的吞吐量会更高一些，但是在工程实践中这种差距几乎感觉不出来。 在极端TPS情况下，采用Leaderless架构的EPaxos、WPaxos协议能做到更高的TPS，但是EPaxos工程实现难度远高于Raft协议，EPaxos协议需要处理复杂的请求之间的依赖关系以实现并行处理请求。在常规分布式系统中，稳定简单的Raft协议完全够用。 在工程实践中，以下因素更能影响协议最后的吞吐量：  网络延迟，RTT（Round Trip Time）：这是大部分共识协议最大的瓶颈。 使用批处理（Batch...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/09/07/Lecture%201/" title="凸函数优化笔记（一）"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-07</div><div class="info-item-2">凸函数优化笔记（一）</div></div><div class="info-2"><div class="info-item-1">向量的范数 norm 我们用符号双竖杠来表示向量范数，例如$ ||x|| 来表示向量来表示向量来表示向量 x 的范数。向量的范数是我们可以用来衡量向量“大小”或者“长度”的一个指标。其定义形式如下，我们通常说一个向量的范数。向量的范数是我们可以用来衡量向量“大小”或者“长度”的一个指标。其定义形式如下，我们通常说一个向量的范数。向量的范数是我们可以用来衡量向量“大小”或者“长度”的一个指标。其定义形式如下，我们通常说一个向量 x 的的的 l_p $-norm为： $ ||x||p = (\sum{i=1}^n |x_i|^p)^{\frac{1}{p}} $ 其中$ p $为一个大于1的实数 如果$ p=1 ，那么，那么，那么 x $的范数就是我们熟知的曼哈顿距离的公式： $ ||x||1 = \sum{i=1}^n |x_i| = |x_1| + … + |x_n| $ 如果$ p=2 ，那么，那么，那么 x $的范数就是我们熟知的欧几里得距离，他描述了向量的长度： $ ||x||2 = (\sum{i=1}^n |x_i|^2)^{\frac{1}{2}} =...</div></div></div></a><a class="pagination-related" href="/2025/06/15/numba/" title="numba介绍"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-15</div><div class="info-item-2">numba介绍</div></div><div class="info-2"><div class="info-item-1">numba是一个python的JIT编译器，用numba加速的Python函数可以达到接近于机器码的运行速度。只用在python函数上加上 @jit的装饰器，就可以将该函数编译到本地机器码运行。numba甚至还支持cuda目标，将python代码编译到cuda平台。 numba效果 12345678910111213141516171819202122232425import numbaimport numpy as np# 普通的 Python 函数 - 计算数组平方和def sum_squares(arr):    result = 0.0    for i in range(len(arr)):        result += arr[i] ** 2    return result# 使用 Numba JIT 加速@numba.njit  # 使用 &quot;no-python&quot; 模式获得最大速度def sum_squares_numba(arr):    result = 0.0    for i in range(len(arr)):       ...</div></div></div></a><a class="pagination-related" href="/2025/04/23/%E4%B8%80%E3%80%81%E4%BB%8B%E7%BB%8D/" title="机器学习简介"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-04-23</div><div class="info-item-2">机器学习简介</div></div><div class="info-2"><div class="info-item-1">...</div></div></div></a><a class="pagination-related" href="/2025/04/15/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92's%20Gradient/" title="线性回归梯度推倒"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-04-15</div><div class="info-item-2">线性回归梯度推倒</div></div><div class="info-2"><div class="info-item-1">损失函数和代价函数 在线性回归中，我们通常使用MSE（Mean Square Error, 均方误差） 作为我们算法的损失函数，其基本思想为衡量预测值和真实结果之间的绝对距离。MSE的公式为：$ MSE = \frac{1}{2m} \sum^m_{i=1}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})^2 。同时我们为了防止函数过拟合，我们引入正则表达式。在这里，我们先用L2正则表达式作为例子，L2正则表达式为：。同时我们为了防止函数过拟合，我们引入正则表达式。在这里，我们先用L2正则表达式作为例子，L2正则表达式为：。同时我们为了防止函数过拟合，我们引入正则表达式。在这里，我们先用L2正则表达式作为例子，L2正则表达式为： \frac{\lambda}{2m}\sum^n_{j=1}{w^2_j} $。结合这两项，我们函数的代价函数可以表达为： $ J(\vec{w},b) = \frac{1}{2m} \sum^m_{i=1}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})^2 +...</div></div></div></a><a class="pagination-related" href="/2025/06/08/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92's%20Gradient/" title="逻辑回归梯度推导"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-08</div><div class="info-item-2">逻辑回归梯度推导</div></div><div class="info-2"><div class="info-item-1">损失函数和代价函数 我们使用交叉熵损失（Cross Entropy Loss）函数作为逻辑回归算法的损失函数，其公式如下 $ L(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)}) = \begin{cases} -log(f_{\vec{w},b}(\vec{x}^{(i)})) &amp; \text{if y=1} \ -log(1-f_{\vec{w},b}(\vec{x}^{(i)})) &amp; \text{if y=0} \ \end{cases} $ 其中, $ f_{\vec{w},b}(\vec{x}^{(i)}) $为 Sigmoid 函数，通过计算给出的输入和自身权重乘积（以及加上偏置），经过Sigmoid函数的处理，最后计算出一个处于(0,1）之间的概率，这个概率表示预测为真的概率。Sigmoid函数公式如下： $ \hat{y} = f_{\vec{w}, b}(\vec{x}^{(i)}) = Sigmoid(\vec{x}^{(i)}) = \frac{1}{1+e^{-(\vec{w}^T \cdot \vec{x}...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/images/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">nty</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">16</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">19</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Nie-Tianyi"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">1.</span> <span class="toc-text">正向传播</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">2.</span> <span class="toc-text">反向传播</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/07/Lecture%201/" title="凸函数优化笔记（一）">凸函数优化笔记（一）</a><time datetime="2025-09-07T08:36:27.000Z" title="发表于 2025-09-07 16:36:27">2025-09-07</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/03/Raft%E5%8D%8F%E8%AE%AE/" title="Raft协议">Raft协议</a><time datetime="2025-09-03T08:36:27.000Z" title="发表于 2025-09-03 16:36:27">2025-09-03</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/08/17/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C's%20Gradient/" title="神经网络梯度推导">神经网络梯度推导</a><time datetime="2025-08-17T08:36:27.000Z" title="发表于 2025-08-17 16:36:27">2025-08-17</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/26/PostgreSQL%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/" title="PostgreSQL使用指南">PostgreSQL使用指南</a><time datetime="2025-06-26T08:36:27.000Z" title="发表于 2025-06-26 16:36:27">2025-06-26</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/23/PyO3/" title="PyO3简介">PyO3简介</a><time datetime="2025-06-23T08:36:27.000Z" title="发表于 2025-06-23 16:36:27">2025-06-23</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By nty</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>